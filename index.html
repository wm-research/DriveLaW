<!DOCTYPE html>
<html>
<head>

  <meta charset="utf-8">
  <meta name="description"
        content="DriveLaW: Unifying Planning and Video Generation in a Latent Driving World">
  <meta name="keywords" content="Gaussians, Dynamic, Rendering, Splatting">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DriveLaW: Unifying Planning and Video Generation in a Latent Driving World</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">


  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/hust.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <!-- <h1 class="title is-1 publication-title">RAD: Training an End-to-End Driving Policy via Large-Scale</h1>
          <h1 class="title is-1 publication-title"> 3DGS-based Reinforcement Learning</h1> -->
          <h1 class="title is-1 publication-title" style="white-space: nowrap;">
            DriveLaW: 
          </h1>
          <h1 class="title is-1 publication-title" style="white-space: nowrap;">
            Unifying Planning and Video Generation in a Latent Driving World
          </h1>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a>Tianze Xia</a><sup>1,2,*</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=F7hPv1QAAAAJ&hl=zh-CN&oi=ao">Yongkang Li</a><sup>1,2,*</sup>,</span>
            <span class="author-block">
              <a>Lijun Zhou</a><sup>2,*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=4qc1qJ0AAAAJ&hl=zh-CN&oi=ao">Jingfeng Yao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a>Kaixin Xiong</a><sup>2</sup>,
            </span>
            <p></p>
            <span class="author-block">
              <a>Haiyang Sun</a><sup>2,†</sup>,
            </span>
            <span class="author-block">
              <a>Bing Wang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a>Kun Ma</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a>Guang Chen</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a>Hangjun Ye</a><sup>2</sup>,
            </span>
            <p></p>
            <span class="author-block">
              <a href="http://eic.hust.edu.cn/professor/liuwenyu/">Wenyu Liu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://xwcv.github.io/">Xinggang Wang</a><sup>1,✉</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Huazhong University of Science &amp; Technology</span>
            <span class="author-block"><sup>2</sup>Xiaomi EV</span>
            <p></p>
            <span class="author-block"><sup>*</sup>Equal Contributions.</span>
            <span class="author-block"><sup>†</sup>Project Leader.</span>
            <span class="author-block"><sup>✉</sup>Corresponding Author.</span>

          <!-- </div> -->
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2512.23421"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://wm-research.github.io/DriveLaW/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>

  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            World models have become crucial for autonomous driving, as they learn how scenarios evolve over time to address the long-tail challenges of the real world. However, current approaches relegate world models to limited roles: they operate within ostensibly unified architectures that still keep world prediction and motion planning as decoupled processes. To bridge this gap, we propose DriveLaW, a novel paradigm that unifies video generation and motion planning. By directly injecting the latent representation from its video generator into the planner, DriveLaW ensures inherent consistency between high-fidelity future generation and reliable trajectory planning. Specifically, DriveLaW consists of two core components: DriveLaW-Video, our powerful world model that generates high-fidelity forecasting with expressive latent representations, and DriveLaW-Act, a diffusion planner that generates consistent and reliable trajectories from the latent of DriveLaW-Video, with both components optimized by a three-stage progressive training strategy. The power of our unified paradigm is demonstrated by new state-of-the-art results across both tasks. DriveLaW not only advances video prediction significantly, surpassing best-performing work by 33.3% in FID and 1.8% in FVD, but also achieves a new record on the NAVSIM planning benchmark.<br><br><br>
          </p>
        </div>
      </div>
    </div>
  </div>

</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3" style="white-space: nowrap; display: inline-block;">Motivation</h2>
    <div class="content has-text-justified">
      VGM (Video Generation Model) latent features can serve as more efficient and informative conditions for action learning. We visualize and compare three types of latent representations. We apply PCA (Principal Component Analysis) to project each representation to 3 principal components mapped to RGB channels, all upsampled to 1280×704. The visualization clearly shows that BEV and VLM features are diffuse, unstable, and exhibit irregular focus patterns. In contrast, VGM features are sharper, less noisy, and demonstrate superior semantic coherence with strong spatial structure awareness, even under challenging driving conditions. This suggests that VGM features provide a more suitable representation for action learning in autonomous driving.
    </div>
    <img src="./static/images/feature.png">
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3" style="white-space: nowrap; display: inline-block;">Framework</h2>
    <div class="content has-text-justified">
      DriveLaW is a unified framework composed of a DriveLaW-Video and a DriveLaW-Act. The video model first encodes past driving frames with a spatiotemporal VAE and encodes textual prompts with a text encoder. A stack of Video DiT blocks then performs latent-space denoising, and the VAE decoder reconstructs the video. Concurrently, action noise, ego status, and high-level commands are encoded and fed into the action model. Video latents from the Video DiT serve as conditioning signals, guiding the Action DiT to output the final trajectory. The Video DiT and Action DiT are chained and trained to learn driving representations from large-scale video generation, providing a shared basis for planning.
    </div>
    <img src="./static/images/drivelaw-framework.png">
  </div>
</section>

 <!-- style="text-align: center;" -->

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3" style="white-space: nowrap; display: inline-block;">Common Video Generation on nuScenes</h2>
    
    <!-- 修改：垂直布局，间距40px，内容居中 -->
    <div style="display: flex; flex-direction: column; gap: 40px; margin-top: 20px; align-items: center;">
    
      <!-- 第一个视频块 -->
      <div style="text-align: center; width: 100%;">
        <h3 class="title is-5">Clip1</h3>
        <video controls width="100%">
          <source src="./static/videos/scene_0001_window_000_comparison_converted.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>

      <!-- 第二个视频块 -->
      <div style="text-align: center; width: 100%;">
        <h3 class="title is-5">Clip2</h3>
        <video controls width="100%">
          <source src="./static/videos/scene_0003_window_007_comparison_converted.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>

      <!-- 第三个视频块 -->
      <div style="text-align: center; width: 100%;">
        <h3 class="title is-5">Clip3</h3>
        <video controls width="100%">
          <source src="./static/videos/scene_0007_window_002_comparison_converted.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>

      <!-- 第四个视频块 -->
      <div style="text-align: center; width: 100%;">
        <h3 class="title is-5">Clip4</h3>
        <video controls width="100%">
          <source src="./static/videos/scene_0014_window_020_comparison_converted.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>

    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3" style="white-space: nowrap; display: inline-block;">Rainy Video Generation on nuScenes</h2>
    
    <!-- 修改：垂直布局，间距40px，内容居中 -->
    <div style="display: flex; flex-direction: column; gap: 40px; margin-top: 20px; align-items: center;">
    
      <!-- 第一个视频块 -->
      <div style="text-align: center; width: 100%;">
        <h3 class="title is-5">Clip1</h3>
        <video controls width="100%">
          <source src="./static/videos/scene_0070_window_014_comparison_converted.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>

      <!-- 第二个视频块 -->
      <div style="text-align: center; width: 100%;">
        <h3 class="title is-5">Clip2</h3>
        <video controls width="100%">
          <source src="./static/videos/scene_0074_window_018_comparison_converted.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>

    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3" style="white-space: nowrap; display: inline-block;">Qualitative results on the Navtest benchmark</h2>
    <div class="content has-text-justified">
      We present representative cases from the Navtest splits, highlighting DriveLaW’s ability to predict future trajectories while ensuring safety and smoothness.
    </div>
    <img src="./static/images/navsim-qual.png">
  </div>
</section>

<!-- <section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3" style="white-space: nowrap; display: inline-block;">Weather-Controllable Generation on Private Data</h2>
    <video controls width="100%">
      <source src="./static/videos/private_weather_shift.mp4" type="video/mp4">
    </video>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Video Generation Results</h2>
    <div class="content has-text-justified">
      Quantitative evaluation of video generation on the NuScenes validation set. Our method outperforms prior single-view state-of-the-art methods in generation quality.
    </div>
    <img src="./static/images/video-gen.png">
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Planning Results</h2>
    <div class="content has-text-justified">
      Performance comparison on NAVSIM Navtest using closed-loop metrics. Methods are grouped by whether they employ an explicit world model: Traditional End-to-End Methods and World Model Methods.† denotes methods trained with the same flow-matching objective.
    </div>
    <img src="./static/images/navsim.png">
  </div>
</section>

<style>
  .container.fixed-width {
    max-width: 1000px; 
    margin: 0 auto;
  }
  .title-container {
    padding-left: 0px;  
  }
  .columns {
    margin-top: 20px; 
  }
</style>

<head>
  <style>
    .video-container-wrapper {
      background-color: #f0f0f0; 
      padding: 20px; 
      border-radius: 5px; 
    }

    .video-container-wrapper video {
      width: 100%; 
      border-radius: 8px; 
    }

    .columns {
      margin-bottom: 20px; 
    }

    .background-container {
      background-color: #f0f0f0; 
      padding: 30px; 
      border-radius: 15px; 
    }
  </style>
</head>



<div style="margin-top: 30px;"></div>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">BibTeX</h2>
    <pre><code>@misc{xia2025drivelawunifyingplanningvideogeneration,
      title={DriveLaW:Unifying Planning and Video Generation in a Latent Driving World}, 
      author={Tianze Xia and Yongkang Li and Lijun Zhou and Jingfeng Yao and Kaixin Xiong and Haiyang Sun and Bing Wang and Kun Ma and Hangjun Ye and Wenyu Liu and Xinggang Wang},
      year={2025},
      eprint={2512.23421},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2512.23421}, 
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link"
         href="https://arxiv.org/abs/2310.08528">
        <i class="fas fa-file-pdf"></i>
      </a>  -->
      <a class="icon-link" href="https://github.com/hustvl" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            The source code is borrowed from <a
              href=""></a>,
              we thank the authors for sharing the templates.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>